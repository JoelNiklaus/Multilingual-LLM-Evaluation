# Multilingual-LLM-Evaluation

Repo for evaluating multilingual LLMs using
Eleuther's [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness).

## Setup

Set up the environment with: `./install_llm_eval.sh`

Run the evaluation with: `./run_llm_eval.sh`

Aggregate the outputs into a csv with: `python analyze_outputs.py`

Visualize the results with: `python visualize_results.py`

